# 如何根据数据范围估计题目允许的时间复杂度，从而估计要用什么算法？

一般计算机每秒能执行约 `10^8` 次运算（`Python` 可能要除以 `10`），可以据此估计能通过的时间复杂度，如下表所示

| 数据范围 | 允许的时间复杂度 | 适用算法举例 |
| --- | --- | --- |
| n ≤ 10 | O(n!) 或 O(C^n) | 回溯、暴力搜索 |
| n ≤ 20 | O(2^n) | 状态压缩 DP |
| n ≤ 40 | O(2^(n/2)) | 折半枚举 |
| n ≤ 10^2 | O(n^3) | 三重循环的 DP、Floyd |
| n ≤ 10^3 | O(n^2) | 二重循环的 DP、背包 |
| n ≤ 10^5 | O(nlogn) | 大多数题目都是这个范围，所以各类算法都有 |
| n ≤ 10^6 | O(n) | 线性 DP、滑动窗口 |
| n ≤ 10^9 | O(n) | 判断质数 |
| n ≤ 10^18 | O(logn) 或 O(1) | 二分、快速幂、数学公式 |

> 注：实际做题时，注意常数因子的影响，如哈希表比数组慢

# 字符串

## 回文串

回文串就是正着读和反着读都一样的字符串

> 找回文串的难点在于：回文串的的长度可能是`奇数`也可能是`偶数`
> 
> 中心扩展法是一种非常直观且高效的方法，兼顾了时间效率与空间节省，非常适合用于找回文串问题

## Manacher's Algorithm

一个更巧妙的解法，`Manacher's Algorithm（马拉车算法）`，时间复杂度只需 `O(n)`

`Manacher’s Algorithm` 是一种高效的算法，用来在 `O(n)` 的时间复杂度内解决最长回文子串问题。相比于其他常规算法（如中心扩展法和动态规划），`Manacher’s Algorithm` 通过对字符串进行预处理，使得能够在线性时间内找到最长的回文子串

主要思想：
- 预处理字符串：在每个字符的两侧都插入特殊字符（如 `#`），这样可以将所有奇数长度和偶数长度的回文统一为奇数长度
- 使用数组记录回文半径：
  - 使用一个数组 `p` 来记录每个位置的回文半径（表示该字符为中心的回文串向两侧扩展的长度）
  - 初始化变量 `center` 和 `right`，分别表示当前已知的回文子串的中心和该回文子串的右边界
- 动态更新回文半径：
  - 遍历预处理后的字符串，根据对称性关系更新每个位置的回文半径：若当前 `i` 位于当前最长回文的右边界 `right` 之内，则 `p[i]` 至少等于 `p[mirror]`，其中 `mirror` 是 `i` 关于 `center` 的对称点；然后尝试进一步扩展以 `i` 为中心的回文串，并根据扩展的结果更新 `p[i]`
  - 如果 `i` 位置的回文串超出了 `right`，则更新 `center` 和 `right` 的值
- 计算最长回文子串的长度和起始位置：遍历数组 `p`，找到最大半径的值，从而计算最长回文子串的长度和在原字符串中的起始位置

> `Manacher's Algorithm（马拉车算法）`是非常高效的算法，通过将所有回文子串转化为`奇数长度`，并利用`对称性`加速半径的计算，该算法可以轻松处理长字符串

# 滑动窗口

## 定长滑动窗口

总结成三步：入 - 更新 - 出
- `入`：下标为 `i` 的元素进入窗口，更新相关统计量
- `更新`：更新答案，`一般是更新最大值/最小值`
- `出`：下标为 `i−k+1` 的元素离开窗口，更新相关统计量

以上三步适用于所有定长滑窗题目

```python
class Solution:
    def maxVowels(self, s: str, k: int) -> int:
        res = count = 0
        for i, c in enumerate(s):
            # 1. 进入窗口
            if c in "aeiou":
                count += 1
            if i < k - 1:  # 窗口大小不足 k
                continue
            # 2. 更新答案
            res = max(res, count)
            # 3. 离开窗口
            if s[i - k + 1] in "aeiou":
                count -= 1
        return res
```

## 基于“滑动窗口 + 字符频率 + valid 计数”的模板

适用场景：
- 判断是否包含某些字符/异位词
- 找到所有异位词
- 最小窗口包含所有字符（如 LeetCode 76）
- 子串满足特定频率要求

通用模板代码（Python）：

```python
from collections import defaultdict

def sliding_window_template(s: str, t: str) -> str:
    need = defaultdict(int)
    window = defaultdict(int)
    for c in t:
        need[c] += 1

    left = right = 0
    valid = 0  # 记录当前窗口满足 need 条件的字符种类数

    # 根据题意定义结果变量，例如最小长度、起始位置、答案列表等
    res = []

    while right < len(s):
        # c 是将要加入窗口的字符
        c = s[right]
        right += 1

        # 更新窗口内数据
        if c in need:
            window[c] += 1
            if window[c] == need[c]:
                valid += 1

        # 判断左侧窗口是否需要收缩
        while right - left >= len(t):  # 对于固定长度匹配，如异位词
        # while valid == len(need):     # 对于所有 need 被满足，如最小覆盖子串
            # 如果满足条件，更新结果
            if valid == len(need):
                res.append(left)  # 对于找异位词，记录当前窗口起点
                # res = s[left:right]    # 对于最小覆盖子串，更新最小结果

            # d 是将要从窗口移除的字符
            d = s[left]
            left += 1

            # 更新窗口内数据
            if d in need:
                if window[d] == need[d]:
                    valid -= 1
                window[d] -= 1

    return res  # 或返回符合条件的子串、长度等
```

使用说明

| 场景 | 如何修改模板 |
| --- | --- |
| 找到所有异位词（如：LeetCode 438）| 保持 `right - left >= len(t)`，每次 `valid == len(need)` 就 `res.append(left)` |
| 判断是否包含异位词（如：LeetCode 567）| 一旦满足 `valid == len(need)` 就 `return True` |
| 最小覆盖子串（如：LeetCode 76）| 条件是 `valid == len(need)`，然后维护 `min_len` 和 `start`，不断缩小窗口 |
| 包含所有字符的最短子串 | 同上，但需在窗口满足后尽量缩小 `left` |

## 滑动窗口的动机从何而来？

| 触发因素 | 对应判断 |
| --- | --- |
| 目标包含连续区间 | 窗口思想可行 |
| 区间大小固定 | 固定长度窗口 |
| 需高效判断区间中是否所有元素满足某条件 | 滑窗 + 条件检查 |
| 可提前剪枝跳过无效区间 | 滑窗结构天然适合 |

> 滑动窗口仅在所有数字都非负时才有效，因为窗口扩展/收缩依赖于总和单调性

## 从两端拿固定长度

这可以使用逆向思维，从两端变成从中间

# 二分搜索

## 普通“二分查找” vs “二分答案”

**传统二分查找**：在一个有序数组里找某个目标值的位置 - 这种情况必须要求数据整体有序，否则无法通过比较缩小搜索区间

**二分答案（又叫 二分可行性 或 Binary Search on the Answer）**：

当我们要找某个最优解（如最大长度 k）时，如果有下面的单调性，就可以用二分：

- 单调性条件：
  - 如果某个长度 `k` 可行（出现次数 ≥ 3），那么更小的长度也一定可行
  - 如果某个长度 `k` 不可行，那么更大的长度也一定不可行

换句话说，随着 `k` 从小到大：
- 出现次数是否 `≥3` 是一个`单调布尔函数`（true→true→…→false→false…）。
- 只要满足这个“单调性”，就可以用二分来快速找到最大可行 `k`，而不需要排序什么数据结构

一句话记忆：
- 传统二分查找需先排序
- 但“二分答案”只需要解的`可行性是单调的`就能用，不需数据排序

# 双指针

## 排序 + 双指针

`排序 + 双指针`是解决三数问题的经典模式，它利用数组的有序性缩小搜索范围，避免冗余计算

逻辑高效清晰，适合在任意三数组合问题中应用

# 快慢指针移除元素

何时使用该模式：
- 从数组中就地移除特定元素
- 在空间受限的情况下过滤数组
- 类似于 283（移动零）、26（移除重复项）、80（最多两个重复项）

## 中位数

中位数本质上是`第 k 小`的数（奇数就是中间第一个，偶数是中间两个平均）

# 图论

## DFS / BFS

`DFS`：找连通块、判断是否有环等
`BFS`：求最短路、扩展传播类问题（多源 BFS）
`DFS+BFS`：求类似“一个区域扩展覆盖另一个区域”的问题，可优先考虑 `DFS+BFS` 的组合方式


## 拓扑排序

把拓扑排序想象成一个黑盒，给它一堆杂乱的先修课约束，它会给你一个井井有条的课程学习安排

这一种在图上的「排序」，可以把杂乱的点排成一排
- 就是把一幅无环图「拉平」，且这个「拉平」的图里面所有箭头方向都是一致的：
  ![alt text](https://github.com/donnapersonal/picx-images-hosting/raw/master/image.6m4610s7ji.webp)
- 前提条件：图中无环，从而保证每条边都是从排在前面的点指向排在后面的点
- 即对于任意有向边 `x→y`，`x` 一定在 `y` 之前

拓扑排序可使用 `DFS` 算法，图的后序遍历结果进行反转就是拓扑排序结果，也可用 `BFS` 算法借助每个节点的入度进行拓扑排序

拓扑排序正是用于处理：
- 有向图
- 依赖关系
- 按顺序处理所有节点，且没有环
- 换句话说，它非常适合解决`课程依赖`这种任务调度类型的问题

> 拓扑排序是对有向无环图 (DAG) 中的节点进行排序，使得对于每条有向边 `u → v`，节点 `u` 在排序中出现在 `v` 之前
> 用于当一个任务依赖于另一个任务时 — 例如，`课程安排`、`构建系统`或`依赖关系解析`

选择解法（`DFS` VS. `BFS` 解法的对比）

| 方法 | 原理 | 特点 | 是否返回顺序 | 是否返回顺序 | 适用 | 核心数据结构 | 时间复杂度 | 空间复杂度 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 拓扑排序（`BFS/Kahn` 算法）| 从入度为0的节点开始，逐步移除边；如果不能遍历所有节点 → 有环 | 递归思路清晰，适合链式依赖 | 可拓扑排序 | 可检测环（3 色状态）| 更倾向层级式任务调度 | 队列 + 入度表 | O(V + E) | O(V + E) |
| `DFS + visited` 状态标记 | 逆后序遍历栈顺序；`DFS` 路径中再次遇到“正在访问”的节点 → 有环 | 更直观，基于入度 | 可拓扑排序 | 可检测环（剩余节点）| 更适合递归链式依赖 | 递归栈 / visited 数组 | O(V + E) | O(V + E) |

## 最短路算法

![alt text](https://github.com/donnapersonal/picx-images-hosting/raw/master/image.5j4khnonz0.webp)

> - 稠密图：边的数量级和 `n^2` 相当的图
> - 稀疏图：边的数量远小于 `n^2` 的图

一个大体使用场景的分析：
- 如果遇到`单源且边为正数`，直接 `Dijkstra`
  - 至于使用`朴素版`还是`堆优化版`还是取决于图的稠密度，多少节点多少边算是稠密图、多少算是稀疏图，这个没有量化，如果想量化只能写出两个版本然后做实验去测试，不同的判题机得出的结果还不太一样
  - 一般情况下，可以直接用`堆优化`版本
  
- 如果遇到`单源边可为负数`，直接 `Bellman-Ford`，同样 `SPFA` 还是 `Bellman-Ford` 取决于图的稠密度
  - 一般情况下，直接用 `SPFA`
  - 如果有`负权回路`，优先 `Bellman-Ford`，如果是`有限节点最短路`也优先 `Bellman-Ford`，理由是写代码比较方便
  > - `Bellman-Ford` 本身就是设计用来在固定步数内找最短路径的，尤其适合这题 —— 不用建图、无环路径、保证能处理 `K` 约束
  > - `Bellman-Ford` 算法适用于“边的数量有限制”的场景，这里只允许走 `k + 1` 条边，所以最多执行 `k + 1` 轮松弛操作即可

- 如果是遇到`多源点求最短路`，直接 `Floyd`
  - 除非`源点特别少且边都是正数`，那可以`多次 Dijkstra`求出最短路径，但这种情况很少，一般出现多个源点了，就是考虑用 `Floyd` 了
  
- 对于`A*`，由于其高效性，所以在实际工程应用中使用最为广泛，由于其结果的不唯一性，即可能是次短路的特性，一般不适合作为算法题
  - 游戏开发、地图导航、数据包路由等都广泛使用 `A*` 算法

## Union-Find

树的定义是无环的连通图，具有以下性质：
- 图中没有环
- 图是连通的（即所有节点都可以相互到达）

> 而判断两个节点是否连通（是否在同一个连通分量中）就是 `Union-Find` 并查集算法的拿手绝活

要判断一个无向图是否是树，可以使用以下条件：
- 边的数量：如果图是树，那么它有 `n` 个节点和 `n-1` 条边；如果边数不等于 `n-1`，则直接返回 `False`
- 连通性和无环性：可以使用`并查集`或`深度优先搜索 (DFS)`来判断
  - 并查集用于检测环：如果添加一条边导致两个节点的根相同，则出现了环
  - 并查集还可以帮助检查连通性

并查集方法的优点：

| 要素 | DFS/BFS | Union-Find |
| --- | --- | --- |
| 循环检测 | 需要严谨的逻辑 | 使用 find 就可简单检测 |
| 连通性    | 须跟踪访问次数 | 若边数 = n-1，则可保证 |
| 时间复杂度 | O(n + e) | O(nα(n)) ≈ O(n) |
| 空间复杂度 | Medium | Very clean |

## 欧拉路径

涉及到 `Hierholzer` 算法

当被要求构建一条路径，使每条边恰好使用一次 — 这本质上是在有向图中寻找一条欧拉路径

可用`DFS + 回溯`模拟 `Hierholzer` 算法（后序入栈）

## Floyd 算法

全源最短路：Floyd 算法

Floyd-Warshall 是处理「所有点对最短路径」的经典方法，适用于 `n` 不大的`稠密图`

## Dijkstra 算法

适合边权为正的图中求单源最短路径，适合图的单源最短路径

单源最短路：Dijkstra 算法

> - 邻接表+Dijkstra：O(n(n+m))
> - 邻接矩阵+Dijkstra：O(n^2)
> - 堆 + Dijkstra：O(mlogm)
> - Bellman_Ford：O(nm)
> - SPFA：O(km−nm)

可以对每一个节点求解单源最短路，即某一个节点到其它所有节点的最短距离

与 `Floyd-Warshall` 相比，`Dijkstra` 更适合`稀疏图`，节省空间与时间

## 最小生成树

涉及到 Kruskal 算法和 Prim 算法

## 连通块

> - 连通分量：任意两点间有路径可达，且不和其他分量连通
> - 完整分量：一个连通分量中，任意两个节点之间都有边（即形成一个完全图）

寻找连通块：`DFS/BFS/Union-Find`

# 栈

`括号匹配是使用栈解决的经典问题`，由于栈结构的特殊性，非常适合做对称匹配类的题目
> 编译器在词法分析过程中处理括号、花括号等这个符号的逻辑，也是使用了`栈`这种数据结构

使用栈来处理嵌套的编码问题

# 队列

## 单调队列

单调队列 = 滑动窗口 + 单调栈

入队、出队、更新答案，这三步的顺序如何思考？- 有两种情况
- 如果更新答案时，用到的数据包含当前元素，则就需先入队，再更新答案
- 如果用到的数据不包含当前元素，则就需要先更新答案，再入队
- 至于出队，一般写在前面，每遍历到一个新的元素，就看看队首元素是否失效（不满足要求），失效则弹出队首

单调队列（Monotonic Queue）的典型用途：
- 维护一个区间的最小值/最大值
- 保证每次都能以 `O(1)` 拿到最优解
- 每个状态最多入队和出队一次，整个过程总共是 `O(n)`

# 树

## 递归

递归本质上就是`深度优先搜索（DFS）`的一种形式

> 递归法 & DFS
> - 递归（Recursion）是一种 实现方式，它利用函数调用自身来遍历结构化数据（如树）
> - DFS（Depth-First Search，深度优先搜索）是一种 遍历策略，指的是 先往深处走到底，再回溯，适用于树和图的遍历

> 链表天然具有递归结构

## Morris 中序遍历

`Morris` 遍历算法是另一种遍历二叉树的方法，它能将非递归的中序遍历空间复杂度降为 `O(1)`

> `Morris` 遍历使用`线索二叉树（Threaded Binary Tree）`，让树本身暂时充当栈来存储路径信息

`Morris` 遍历算法整体步骤如下（假设当前遍历到的节点为 `cur`）
- 如果 `cur` 无左孩子，先将 `cur` 的值加入答案数组，再访问 `cur` 的右孩子，即 `cur=cur.right`
- 如果 `cur` 有左孩子，则找到 `cur` 左子树上最右的节点（即左子树中序遍历的最后一个节点，`cur` 在中序遍历中的前驱节点），记为 `predecessor`。根据 `predecessor` 的右孩子是否为空，进行如下操作
  - 如果 `predecessor` 的右孩子为空，则将其右孩子指向 `cur`，然后访问 `cur` 的左孩子，即 `cur=cur.left`
  - 如果 `predecessor` 的右孩子不为空，则此时其右孩子指向 `cur`，说明我们已经遍历完 `cur` 的左子树，我们将 predecessor 的右孩子置空，将 `cur` 的值加入答案数组，然后访问 `cur` 的右孩子，即 `cur=cur.right`
- 重复上述操作，直至访问完整棵树

![alt text](https://github.com/donnapersonal/picx-images-hosting/raw/master/morris.13lwj1eaxs.webp)

## 平衡二叉树

二叉搜索树（BST）特点：
- 左子树的所有节点都小于根节点
- 右子树的所有节点都大于根节点
  
平衡二叉树（height-balanced BST）定义：一个二叉树称为平衡的，当它的任意节点的左右子树的高度差不超过 `1`

## Trie

`Trie` 是一种树状结构：
- 每个节点代表一个字符
- 从根节点到某个节点的路径组成一个前缀
- 特别适合处理大量字符串的前缀匹配问题

# 贪心

## 贪心策略

有两种基本贪心策略：
- 从最小/最大开始贪心，优先考虑最小/最大的数，从小到大/从大到小贪心。在此基础上，衍生出了反悔贪心
- 从最左/最右开始贪心，思考第一个数/最后一个数的贪心策略，把 n 个数的原问题转换成 n−1 个数（或更少）的子问题

对于无法排序的题目，尝试从左到右/从右到左贪心
- 思考第一个数/最后一个数的贪心策略，把 `n` 个数的原问题转换成 `n−1` 个数（或更少）的子问题

## 区间贪心

区间贪心有如下经典问题：
- `不相交区间（单机器调度/活动安排）`：给定一些区间，从中选出尽量多的两两互不相交的区间
- `区间分组（任务调度/会议室）`：给定一些区间，把这些区间分成最少的组，使得每组内的区间互不相交
- `区间选点（射气球，Interval Stabbing）`：给定一些区间，在数轴上放置最少的点，使得每个区间都包含至少一个点。最少要放置多少个点？
- `区间覆盖（灌溉花园）`：给定一些区间，从中选出尽量少的区间，覆盖一条指定线段 `[s,t]`

## 字符串贪心

**字典序最小/最大**
字典序的定义如下：
- 对于两个字符串 `a` 和 `b`，从左到右依次比较 `a[i]` 和 `b[i]` 的字符 `ASCII` 值的大小
- `a[i] 不等于 b[i]` 时，如果 `a[i]<b[i]`，那么 `a` 的字典序更小，否则 `b` 的字典序更小
- 如果没有出现 `a[i] 不等于 b[i]`，则短的字符串字典序更小
- 如果两个字符串的长度和内容均相同，那么两个字符串的字典序一样
  
字典序的定义也可以推广到数组上，按照上述方法比较两个数组的字典序

# 数学

## 如何遍历一个整数

如果不把 `x` 转成字符串，要怎么做？

可以不断地取 `x` 的最低位（`模 10`），去掉 `x` 的最低位（`除以 10`），直到 `x=0`

如 `x=123`：
- 通过 `x mod 10` 取到个位数 `3`，然后把 `x` 除以 `10`（下取整），得到 `x=12`
- 再次 `x mod 10` 取到十位数 `2`，然后把 `x` 除以 `10`（下取整），得到 `x=1`
- 最后 `x mod 10` 取到百位数 `1`，然后把 `x` 除以 `10`（下取整），得到 `x=0`，此时完成遍历退出循环

## 如何反转一个整数

比如现在有一个数 `56`，如何把 `7` 加到 `56` 的末尾？
- 把 `56` 乘以 `10`，再加上 `7`，就得到了 `567`
- 一般地，如果要把数字 `b` 加到整数 `a` 的末尾，可以计算 `a⋅10+b`

如果要把 `x=123` 反转，我们可以按照前置知识一中的方法，从低到高遍历 `x` 的每一位，即 `3,2,1`

初始化 `rev=0`，依次把 `3,2,1` 加到 `rev` 的末尾，即：
- 更新 `rev` 为 `rev⋅10+3=3`
- 更新 `rev` 为 `rev⋅10+2=32`
- 更新 `rev` 为 `rev⋅10+1=321`
- 最终得到了 `x` 反转后的结果 `rev=321`

## 三角形成立条件

三角形成立条件：设三边为 `a ≤ b ≤ c`，成立三角形的条件是 `a + b > c`

这就是「三角形不等式」：`任意两边之和 > 第三边`

## 质数

> 质数的定义：大于 `1` 且只能被 `1` 和自身整除的正整数

# 位运算

位运算的关键性质：

| 表达式 | 结果说明 |
| --- | --- |
| `a ^ 0 = a` | 任意数与 `0` 异或仍为它本身 |
| `a ^ a = 0` | 相同的两个数异或为 `0` |
| `a ^ b ^ a = b` | `XOR` 是可交换的和结合的：顺序并不重要，可以交换顺序，`a` 被抵消 |

> 异或运算还可用于“检测差异”
> - A ^ B = 1 代表两个值在该位不同
> - 所以 i ^ (i >> 1) 实际上是在构造出每位相对于前一位是否变化的标志位

一些位运算操作：
| 操作 | 结果 |
| --- | --- |
| `n & 1` | 取 `n` 的最低位 |

## Brian Kernighan 算法

还有一个位移相关的算法叫做 `Brian Kernighan 算法`，它用于清除二进制串中最右边的 `1`

`Brian Kernighan` 算法的关键在于每次对 `number` 和 `number−1` 之间进行按位与运算后，`number` 中最右边的 `1` 会被抹去变成 `0`

# 排序

几个常用的经典排序算法对比：

| 算法 | 时间复杂度 | 空间复杂度 | 是否稳定 | 最坏性能 |
| --- | --- | --- | --- | --- |
| 快速排序 | O(nlogn) | O(log n) | 否    | O(n²) |
| 归并排序 | O(nlogn) | O(n) | 是 | O(nlogn) |
| 堆排序 | O(nlogn) | O(1) | 否  | O(nlogn) |

## Heap sort VS. Merge sort

| 指标 | Merge Sort | Heap Sort |
| --- | --- | --- |
| 最差测试是否超时 TLE | 从未超时 | 从未超时，但接近限制 |
| 内存使用 | 略高，O(n) 临时空间 | O(1)，常数额外空间 |
| 排序稳定性 | 稳定（顺序不变）| 不稳定（重复值可能换顺序）|
| 重复值适应性 | 极佳（不会退化）| 良好 |
| 实现复杂度 | 中（带辅助数组）| 中等（堆结构操作）|
| 最坏性能表现 | O(n log n)，极其稳定 | O(n log n)，但实际慢一点 |
| 适合面试使用 |（性能 + 表现兼顾）|（空间优势明显）|

分析总结
- `Merge Sort` 优势：
  - 在 LeetCode 上运行最稳定
  - 适合大数据量 + 重复值
  - 最坏情况不 TLE
  - 对初学者来说可读性和推理过程更容易讲清楚
  - 排序稳定性好（如用于对象数组时尤为重要）
- `Heap Sort` 劣势：
  - 实际运行时间较长（多了许多 swap 操作）
  - 不是稳定排序
  - 尽管 O(1) 空间，但对 Python3 中频繁的 swap 不太友好（开销大）
  - 在 LeetCode 这类有极限测试的平台上表现略弱

最佳实践建议（按场景选择
| 需求场景 | 推荐算法 | 原因 |
| --- | --- | --- |
| LeetCode 大规模测试、稳定性优先 | ✅ Merge Sort | 不易超时，时间复杂度稳定 |
| 空间敏感、嵌入式设备排序 | ✅ Heap Sort | 原地排序 + 空间常数 |
| 追求极限速度（LeetCode top）| Python 内置 sort() | 使用 Timsort，最优性能、缓存优化 |
| 面试讲解、展示思路能力 | Merge Sort / Heap Sort | 两者都行，Merge 更适合讲 divide-and-conquer |

# 堆

堆通常使用`数组`表示，在数组中构建堆的规则（基于`完全二叉树`的性质），对于一个使用 `0-based` 数组表示的堆（即根节点是索引 `0`），对于索引 `i`：
- 左子节点：`2*i + 1`
- 右子节点：`2*i + 2`
- 父节点：`(i - 1) // 2`

如何找到最后一个非叶节点？
- 对于数组长度为 `n` 的堆，叶子节点从索引 `n // 2` 到 `n - 1`
- 因此，最后一个非叶节点的索引是：`(n // 2) - 1`

# 回溯

> 本质是搜索树上的 `DFS`

回溯是一种尝试所有可能组合的算法，尤其适合生成排列、组合、子集等问题

# 懒删除/懒修改

**懒删除（Lazy Deletion）**

当要“删除”一个元素时，不直接从原数据结构中把它物理删除（因为可能代价高），而是先在辅助结构里打一个“已删除”标记，等到真正取用或遍历时再判断它是否有效，如果无效就跳过

为什么需要懒删除？
- 以 堆（`heap`）为例：
  - 堆支持 `O(log n)` 插入、`O(log n)` 弹出堆顶
  - 但若你想删除堆里任意位置的元素（不是堆顶），需先找它的位置（`O(n)`），再调整堆结构（`O(log n)`）→ 总体很慢
- 所以我们不直接删堆里的元素，而是：
  - 在哈希表里标记它“失效”
  - 当它将来跑到堆顶时，检查有效性，如果已失效就 `pop` 丢掉，不做处理

这样就避免了堆里任意位置的 `O(n)` 删除

**懒修改（Lazy Update）**

当我们需要修改堆中某个元素的值（例如优先级），由于无法在堆中直接高效定位并修改这个元素，我们不删除旧版本，而是插入一个新版本，并将旧版本视为“无效”，在后续弹出时跳过它

为什么使用懒修改？
- 在 `Python` 的 `heapq`（或任何二叉堆）中：
  - 无法直接修改堆中间的任意元素，因为堆是结构化的，找元素 `O(n)`
  - 直接修改 + 重建堆开销太大
- 懒修改解决方案：
  - 直接插入一个 新版本的任务（优先级已更新）
  - 把旧版本丢在堆里，不管它
  - 当旧版本 `later` 被弹出时，用辅助哈希表（如 `mp`）检测是否为最新 → 如果不是就跳过